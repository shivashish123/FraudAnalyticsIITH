{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data from the csv file and removing the outliers and then splitting into train and test sets\n",
    "\n",
    "Z1 = pd.read_csv('clr-data.csv')\n",
    "Z1 = Z1[Z1['FNC']<1e6]\n",
    "Z = Z1[Z1['FNC']>0]\n",
    "X = Z.drop(['Status'],axis=1)\n",
    "Y = Z['Status']\n",
    "X_temp_train, X_temp_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=1)\n",
    "FNC_train = X_temp_train['FNC']\n",
    "FNC_test = X_temp_test['FNC']\n",
    "X_train = X_temp_train.drop(['FNC'],axis=1)\n",
    "X_test = X_temp_test.drop(['FNC'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for standardizing the data using mean and standard deviation\n",
    "\n",
    "def standardize(train, test):\n",
    "    mean = np.mean(train, axis=0)\n",
    "    std = np.std(train, axis=0)+0.000001\n",
    "    X_train = (train - mean) / std\n",
    "    X_test = (test - mean) /std\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = standardize(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the costs\n",
    "cost_TN = 0\n",
    "cost_TP = 150\n",
    "cost_FP = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the false negative cost for each row will require merging the cost into the output \n",
    "\n",
    "def create_y_input(y_train, c_FN):\n",
    "    y_str = pd.Series(y_train).reset_index(drop=True).apply(lambda x: str(int(x)))\n",
    "    c_FN_str = pd.Series(c_FN).reset_index(drop=True).apply(lambda x: '0'*(6-len(str(int(x)))) + str(int(x)))\n",
    "    return y_str + '.' + c_FN_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the customised loss function\n",
    "\n",
    "def custom_loss2():\n",
    "    def loss_function(y_input, y_pred):\n",
    "        y_true = K.round(y_input)\n",
    "        c_FN = (y_input - y_true) * 1e6\n",
    "        cost = (y_true * (1-y_pred) * c_FN) + (y_true * (y_pred) * cost_TP) + ((1 - y_true) * ( y_pred) * cost_FP) +  ( (1 - y_true) * (1-y_pred) * cost_TN)\n",
    "        return K.mean(cost, axis=-1)\n",
    "    return loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the logistic regression model\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating and compiling the model \n",
    "model = create_model()\n",
    "model.compile(optimizer='adam', loss=custom_loss2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "643/643 [==============================] - 1s 961us/step - loss: 174.7203\n",
      "Epoch 2/2\n",
      "643/643 [==============================] - 1s 1ms/step - loss: 101.5322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f86fba52460>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating customised output by merging the False Negative Cost in the output\n",
    "y_train = create_y_input(Y_train,FNC_train)\n",
    "y_train = np.float32(y_train)\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the result\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7724633742619266\n",
      "precision = 0.44137591726105907\n",
      "recall = 0.7282597456312687\n",
      "f1_score = 0.6046531593406593\n"
     ]
    }
   ],
   "source": [
    "#Analysis of the result\n",
    "print(\"accuracy =\",accuracy_score(Y_test,y_pred.round()))\n",
    "print(\"precision =\",average_precision_score(Y_test, y_pred.round()))\n",
    "print(\"recall =\",recall_score(Y_test, y_pred.round()))\n",
    "print(\"f1_score =\",f1_score(Y_test, y_pred.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost savings = 0.7137320615008198\n"
     ]
    }
   ],
   "source": [
    "#computing the savings cost\n",
    "offset = len(Y_train)\n",
    "cost_max=0\n",
    "costs=0\n",
    "for i in range(len(Y_test)):\n",
    "    if(Y_test.iloc[i]==1):\n",
    "        cost_max = cost_max + FNC_test.iloc[i]\n",
    "for i in range(len(Y_test)):\n",
    "    if(Y_test.iloc[i]==1):\n",
    "        if(y_pred[i].round()==1):\n",
    "            costs=costs+cost_TP\n",
    "        else:\n",
    "            costs=costs+FNC_test.iloc[i]\n",
    "    else:\n",
    "        if(y_pred[i].round()==1):\n",
    "            costs=costs+cost_FP\n",
    "        else:\n",
    "            costs=costs+cost_TN\n",
    "            \n",
    "print(\"cost savings =\",1-(costs/cost_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
